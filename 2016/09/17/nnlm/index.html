<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta property="wb:webmaster" content="0f4e330dfccdfaa4" />
  
  
  <meta name="keywords" content="python, data mining, machine learning">
  
  <title>神经网络语言模型-NNLM | Code4Future</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="作者：一个独行的程序员
Bengio 2003年的NNLM可谓是神经网络语言模型的开山之作，并且为后来的RNNLM、Word2Vec、NMT等提供了思路，为DL4NLP打下了坚实的基础。
语言模型通俗的说，语言模型就是判断一句话像不像是人说出来的。形式化的定义，语言模型是借由一个概率分布，试图用概率p(s)来表示字符串s作为1个句子出现的频率：
$$    p(s) = p(w_1,…w_m)$$">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络语言模型-NNLM">
<meta property="og:url" content="http://zjupjt.com/2016/09/17/nnlm/index.html">
<meta property="og:site_name" content="Code4Future">
<meta property="og:description" content="作者：一个独行的程序员
Bengio 2003年的NNLM可谓是神经网络语言模型的开山之作，并且为后来的RNNLM、Word2Vec、NMT等提供了思路，为DL4NLP打下了坚实的基础。
语言模型通俗的说，语言模型就是判断一句话像不像是人说出来的。形式化的定义，语言模型是借由一个概率分布，试图用概率p(s)来表示字符串s作为1个句子出现的频率：
$$    p(s) = p(w_1,…w_m)$$">
<meta property="og:image" content="http://zjupjt.com/2016/09/17/nnlm/nnlm1.png">
<meta property="og:updated_time" content="2016-09-17T05:41:33.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络语言模型-NNLM">
<meta name="twitter:description" content="作者：一个独行的程序员
Bengio 2003年的NNLM可谓是神经网络语言模型的开山之作，并且为后来的RNNLM、Word2Vec、NMT等提供了思路，为DL4NLP打下了坚实的基础。
语言模型通俗的说，语言模型就是判断一句话像不像是人说出来的。形式化的定义，语言模型是借由一个概率分布，试图用概率p(s)来表示字符串s作为1个句子出现的频率：
$$    p(s) = p(w_1,…w_m)$$">
<meta name="twitter:image" content="http://zjupjt.com/2016/09/17/nnlm/nnlm1.png">
  
    <link rel="alternative" href="/atom.xml" title="Code4Future" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <!-link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link href="http://fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>
<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Code4Future</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">I am a slow walker, but I never walk back.</a>
        </h2>
      
    </div>

    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/sitemap.xml">Map</a>
        
      </nav>    

      <nav id="sub-nav">
        
        <a class="nav-icon" href="https://cn.linkedin.com/in/jingtianP" title="LinkedIn" target="_blank">&#61665;</a>
        <a class="nav-icon" href="https://github.com/DjangoPeng" title="GitHub" target="_blank">&#61595;</a>
        <a class="nav-icon" href="http://weibo.com/2806951572" title="weibo" target="_blank">&#61834;</a>
        <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>

      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://zjupjt.com"></form>
      </div>

      <script type="text/javascript">
        (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
        (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
        e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
        })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

        _st('install','Qxi4BvHzoYsUfYh7oiD_','2.0.0');
      </script>
      
    </div>
    <div>
        
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-nnlm" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      神经网络语言模型-NNLM
    </h1>
  

      </header>
    
    <div class="article-meta">
      <a href="/2016/09/17/nnlm/" class="article-date">
  <time datetime="2016-09-17T04:25:45.000Z" itemprop="datePublished">2016-9-17 Sat  12:25</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <p>作者：<a href="https://cn.linkedin.com/in/jingtianp" target="_blank" rel="external">一个独行的程序员</a></p>
<p><em>Bengio 2003年的NNLM可谓是神经网络语言模型的开山之作，并且为后来的RNNLM、Word2Vec、NMT等提供了思路，为DL4NLP打下了坚实的基础。</em></p>
<h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><p>通俗的说，语言模型就是判断一句话像不像是人说出来的。<br>形式化的定义，语言模型是借由一个概率分布，试图用概率p(s)来表示字符串s作为1个句子出现的频率：</p>
<p>$$<br>    p(s) = p(w_1,…w_m)<br>$$</p>
<a id="more"></a>
<p>语言模型在许多NLP相关的问题都有应用，如语音识别、机器翻译、词性标注、句法分析和信息检索等。<br>然而，如果通过穷举的方式枚举所有的w来计算p(s)，时间和空间代价都是巨大的。<br>假设词汇表单词数V = 10000，句子平均长度为M = 20，则时间复杂度为$O(V^M)$，即$O(10^80)$。因此，我们常常使用<strong>n-gram</strong>模型来近似求解p(s)。</p>
<h2 id="n-gram-语言模型"><a href="#n-gram-语言模型" class="headerlink" title="n-gram 语言模型"></a>n-gram 语言模型</h2><p>对于一个由M个词构成的句子$s=w_1w_2…w_M$，其概率计算公式为：</p>
<p>$$<br>    p(s) = p(w_1)p(w_2|w_1)p(w_3|w_1w_2)…p(w_t|w_1…w_{M-1})<br>         = \prod_{i=1}^{M} p(w_i|w_1…w_{i-1})<br>$$</p>
<p>产生第i个词的概率是由已经产生的i-1个词$w_1w<em>2…w</em>{i-1}$决定的。n-gram模型则是只考虑已经产生的前n-1个单词，而不是从句首开始的所有单词。因此，公式变为：</p>
<p>$$<br>    p(s) = \prod_{i=1}^{M} p(w_i|w_{i-n+1}…w_{i-1})<br>         = \prod_{i=1}^{M} p(w_i|w_{i-n+1}^{i-1})<br>$$</p>
<p>在实际建模时，考虑到计算时间，我们常常取n=2或n=3，即bigram或trigram。</p>
<h2 id="Bengio的NNLM"><a href="#Bengio的NNLM" class="headerlink" title="Bengio的NNLM"></a>Bengio的NNLM</h2><p>相对于统计语言模型，NNLM在相似语义（similar semantic）和语法角色（grammatical roles)上进行了优化。首先看一个例句：</p>
<p>“The cat is walking in the bedroom”</p>
<p>通过NNLM可以产生类似的句子，如：</p>
<p>“A dog was running in a room”</p>
<p>因为两句话中的(the, a)、(room, bedroom)、(dog, cat)等词对拥有相似的语义和相同的语法角色，所以才能构造出相似的句子。</p>
<p>同时，NNLM通过分布式表示（distributed representations）解决了维度灾难（curse of dimensionality）的问题。通过词向量矩阵C，将词汇表中的V个单词映射为V个m维的词向量（feature vector）。</p>
<p>同样使用n-gram表示，但是NNLM却是共享参数矩阵C。相反，统计语言模型却需要用词矩阵表示每一个句子，空间代价太大。</p>
<p>Bengio通过1个3层的神经网络来构建NNLM，并且与普通NN不同的是：输入层的输入数据实质上是词向量矩阵C，并且是全局共享的。</p>
<p>为了说明，下面定义符号：<br>训练集是许多由词$w_1…w_T$构成的句子，其中$w_t \in V$，词汇表V是有限个不重复词组成的集合。训练目标是学到一个很好的模型:</p>
<p>$$<br>f(w_t,…,w_{t-n+1})=\hat p(w_t|w_1^{t-1})<br>$$</p>
<p>取 $\frac{1}{\hat p(w_t|w_1^{t-1})}$ 的几何平均值作为困惑度（<em>perplexity</em>）,即log似然的几何平均的指数。</p>
<p>模型中的唯一常数是对于任意的组合$w_1^{t-1}$,保证</p>
<p>$$<br>\sum_{i=1}^{|V|} f(i,w_{t-1},…,w_{t-n+1}) = 1, \quad  f &gt; 0<br>$$</p>
<p>通过条件概率的乘积，我们能够得到许多词组合成句子的联合概率。将</p>
<p>$$<br>f(w_t,…,w_{t-n+1})=\hat p(w_t|w_1^{t-1})<br>$$</p>
<p>分解成两部分：</p>
<ol>
<li>词汇表V中的第i个单词-&gt;词向量$C(i) \in R^m$。 C(i)表示每个词i的分布式词向量(distributed feature vector),C是一个拥有$|V| x m$参数的词向量矩阵。</li>
<li>第i个词的上文的向量形式为：$(C(w_{t-n+1}),…,C(w_{t-1}))$,已知词汇表V中第i个词在已知前n-1个词时的条件概率是$p(w_t|w_1^{t-1})$，定义函数</li>
</ol>
<p>$$<br>g(i,C(w_{t-n+1}),…,C(w_{t-1}))<br>$$</p>
<p>为此时第i个词的词向量，即：</p>
<p>$$<br>f(i,…,w_{t-n+1}) = g(i,C(w_{t-n+1}),…,C(w_{t-1}))<br>$$</p>
<p>因此，函数f是C和g的复合函数，并且C是所有词共享的一个参数矩阵。函数g可以用一个前馈网络(FNN)或循环神经网络(RNN)来实现，设网络中所有参数为$\omega$。则整个NNLM的参数$\theta = (C,\omega) $。</p>
<p>模型的训练目标：</p>
<p>$$<br>    \mathop{argmax}_{\theta} L = \frac{1}{T} \sum_{t}^{|V|} log f(w_t,…,w_{t-n+1}; \theta) + R(\theta)<br>$$</p>
<p>其中$R(\theta)$为正则化惩罚项。</p>
<img src="/2016/09/17/nnlm/nnlm1.png" alt="神经网络语言模型" title="神经网络语言模型">
<p>输入层：将$C(w_{t-n+1}),…,C(w_{t-1})$这n-1个词向量首尾相接拼起来形成一个$(n-1)m$维的向量x。</p>
<p>隐藏层：输入$o=d+Hx$，d为$h$维的隐层偏置项，H为$h \times (n-1)m$维的隐层参数；输出$a=tanh(o)$</p>
<p>输出层：用Softmax做V分类，模型大部分计算都在这一层；$y_i$表示下一个词为i的未归一化log概率：</p>
<p>$$<br>    y = b + Wx + U tanh(d+Hx)<br>$$</p>
<p>其中，U为$|V| \times h$维的输出层参数矩阵，b为$|V|$维的输出层偏置项。模型考虑了从输入层直接到输出层的概率，W即为输入层直连输出层的$|V| \times (n-1）m$维参数矩阵。</p>
<p>综上：$\theta = (b,d,W,U,H,C)$，</p>
<p>总参数个数为:$|V|(1+mn+h)+h(1+(n-1)m)$。</p>
<p>用随机梯度上升求解：</p>
<p>$$<br>    \theta \leftarrow \theta + \epsilon \frac {\partial log \hat P(w_t|w_{t-1},…,w_{t-n+1}) } {\partial \theta}<br>$$</p>
<p>收敛后得到了词向量矩阵C即为我们NNLM中最重要的参数，通过矩阵C可以完成词（Word）到向量（Vector）的转换，后来Mikolov的RNNLM、Continuous Space Word Representation、Word2Vec等都是在Bengio的这篇paper上改进来的。</p>
<h2 id="参考论文"><a href="#参考论文" class="headerlink" title="参考论文"></a>参考论文</h2><p>[1] <a href="chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="external">Yoshua Bengio, R ́ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic languagemodel.The Journal of Machine Learning Research, 3:1137–1155, 2003. </a></p>
<p>[2] <a href="chrome-extension://oemmndcbldboiebfnladdacbdfmadadm/http://arxiv.org/pdf/1310.4546.pdf" target="_blank" rel="external">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS, 2013.</a></p>

      
    </div>
    <footer class="article-footer">
      
        <a data-url="http://zjupjt.com/2016/09/17/nnlm/" data-id="cit6ru9yr000cgphpy4ewqywt" class="article-share-link">分享</a>
      
      
        <a href="http://zjupjt.com/2016/09/17/nnlm/#ds-thread" class="ds-thread-count article-comment-link" data-thread-key="2016/09/17/nnlm/">评论</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Deep-Learning/">Deep Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Neural-Nework/">Neural Nework</a></li></ul>

    </footer>

      
      
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2016/08/13/nn-fp-bp/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">神经网络模型</div>
    </a>
  
</nav>

  


  
    <section id="comments">
    <!-- 多说评论框 start -->
      <div id="ds-thread" class="ds-thread" data-thread-key="2016/09/17/nnlm/" data-title="神经网络语言模型-NNLM" data-url="http://zjupjt.com/2016/09/17/nnlm/"></div>
    <!-- 多说评论框 end --> 
    <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
    <script type="text/javascript">
    var duoshuoQuery = {short_name:"code4future"};
      (function() {
        var ds = document.createElement('script');
        ds.type = 'text/javascript';ds.async = true;
        ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
        ds.charset = 'UTF-8';
        (document.getElementsByTagName('head')[0] 
         || document.getElementsByTagName('body')[0]).appendChild(ds);
      })();
      </script>
    <!-- 多说公共JS代码 end -->
    </section>
  



</article>
</section>
        
          <aside id="sidebar">
  
    <div class="widget-wrap">
	<h3 class="widget-title">About Me</h3>
	<div class="widget">
	  <!-img src="http://pic.4j4j.cn/upload/pic/20130221/c53f922687.jpg" height="80px" width="80px">
	  <p><b>	Jingtian Peng</b></br></>
	  <p align="right"><i> -- A Single Coder </i></>
	</div>
</div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Data-Mining/">Data Mining</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Others/">Others</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Classification/" style="font-size: 10px;">Classification</a> <a href="/tags/Data-Mining/" style="font-size: 10px;">Data Mining</a> <a href="/tags/Deep-Learning/" style="font-size: 16.67px;">Deep Learning</a> <a href="/tags/Information-Retrieval/" style="font-size: 10px;">Information Retrieval</a> <a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a> <a href="/tags/Neural-Network/" style="font-size: 13.33px;">Neural Network</a> <a href="/tags/Neural-Nework/" style="font-size: 10px;">Neural Nework</a> <a href="/tags/Python/" style="font-size: 16.67px;">Python</a> <a href="/tags/Text-Minging/" style="font-size: 10px;">Text Minging</a> <a href="/tags/Web/" style="font-size: 10px;">Web</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/09/">September 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/08/">August 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/07/">July 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/06/">June 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/02/">February 2016</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/01/">January 2016</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2016/09/17/nnlm/">神经网络语言模型-NNLM</a>
          </li>
        
          <li>
            <a href="/2016/08/13/nn-fp-bp/">神经网络模型</a>
          </li>
        
          <li>
            <a href="/2016/08/04/nn-Activation-Function/">神经网络-激活函数</a>
          </li>
        
          <li>
            <a href="/2016/07/24/ML-LogisticRegression/">机器学习算法-Logistic Regression</a>
          </li>
        
          <li>
            <a href="/2016/06/28/Singular-Vector-Decomposition-SVD/">Singular Vector Decomposition(SVD)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
	<div class="outer">
	    <div id="footer-info" class="inner" style="text-align:center;">
		  <table width="100%" border="0">
	        <tr>
	          <td style="text-align:left">
	            Copyright &copy; 2015-2016 Jingtian Peng &nbsp; &nbsp;
	            Powered by <a href="http://hexo.io/" target="_blank">Hexo</a><br>
			  </td>
			  <td style="text-align:right">
			    <div style="font-family: FontAwesome;font-size: 20px;">
			    <a href="http://weibo.com/2806951572" title="weibo" target="_blank">&#61834;</a>&nbsp;
				<a href="https://cn.linkedin.com/in/jingtianP" title="LinkedIn" target="_blank">&#61665;</a>&nbsp;
				<a href="https://github.com/DjangoPeng" title="GitHub" target="_blank">&#61595;</a>&nbsp;
				</div><br>
			    <a href="/sitemap.xml">网站地图</a>&nbsp; &nbsp;
				<a href="/atom.xml">订阅本站</a>&nbsp; &nbsp;
				<a href="mailto:pjt73651@gmail.com" target="_blank">联系博主</a>&nbsp; &nbsp;
			  </td>
	        </tr>
	      </table>
	    </div>
	</div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/sitemap.xml" class="mobile-nav-link">Map</a>
  
</nav>
    

<script src="http://lib.sinaapp.com/js/jquery/2.2.0/jquery-2.2.0.min.js"></script>
<script type="text/javascript">
//<![CDATA[
if (typeof jQuery == 'undefined') {
  document.write(unescape("%3Cscript src='/js/jquery-2.2.0.min.js' type='text/javascript'%3E%3C/script%3E"));
}
// ]]>
</script>

<!-- Baidu Analytics Start --->
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?4fcfa30fdac460a8f30636bb8c97ad89";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<!-- Baidu Analytics End --->

<!-- Baidu Share Start --->
<script>
window_bd_share_config={"common":{"bdSnsKey":{"tsina":"3687385740"},"bdWbuid":2806951572,"bdText":"","bdMini":"2","bdMiniList":["douban","kaixin001","tieba","tsohu","sqq","youdao","qingbiji","mail","linkedin","mshare","copy","print"],"bdPic":"http://www.devchen.com/SharePic.png","bdStyle":"1","bdSize":"24"},"share":{"bdSize":16}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
<!--- Baidu Share End --->


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>